{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "import en_core_web_md\n",
    "# import en_core_web_lg\n",
    "import random\n",
    "from pathlib import Path\n",
    "from spacy.util import minibatch, compounding\n",
    "import tarfile\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "from pandas import ExcelWriter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import timeit\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.util import decaying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(train_data, model_type):\n",
    "    max_batch_sizes = {\"tagger\": 32, \"parser\": 16, \"ner\": 16, \"textcat\": 64}\n",
    "    max_batch_size = max_batch_sizes[model_type]\n",
    "    if len(train_data) < 1000:\n",
    "        max_batch_size /= 2\n",
    "    if len(train_data) < 500:\n",
    "        max_batch_size /= 2\n",
    "    batch_size = compounding(1, max_batch_size, 1.001)\n",
    "    batches = minibatch(train_data, size=batch_size)\n",
    "    return batches\n",
    "def transformPerson(orginalData = None):\n",
    "    texts = [text for text in orginalData['text']]\n",
    "    transformTags = []\n",
    "    for tags in orginalData['tags']:\n",
    "        entity = {'entities': []}\n",
    "        for tag in eval(tags):\n",
    "            if tag['TYPE'] == 'DOCTOR' or tag['TYPE'] == 'PATIENT':\n",
    "                entity['entities'].append((int(tag['start']), int(tag['end']), 'PERSON'))\n",
    "        transformTags.append(entity)\n",
    "    transformedTrain = list(zip(texts, transformTags))\n",
    "    return transformedTrain\n",
    "def findAllEntityTransform(data = None):\n",
    "    uniqueTag = set()\n",
    "    for (_, tags) in data:\n",
    "        for tag in tags['entities']:\n",
    "            uniqueTag.add(tag[2])\n",
    "    return uniqueTag\n",
    "def findOneEntity(data = None, entType = None):\n",
    "    for tags in data['tags']:\n",
    "        for tag in eval(tags):\n",
    "            if tag['TYPE'] == entType:\n",
    "                print(tag)\n",
    "def evaluate(ner_model, examples, n_iter):\n",
    "    stats = ['ents_f']\n",
    "    scorer = Scorer() \n",
    "    for input_, annot in examples:\n",
    "        try:\n",
    "            doc_gold_text = ner_model.make_doc(input_) #the raw text\n",
    "            gold = GoldParse(doc_gold_text, entities=annot['entities']) # combine the raw text with corrected id\n",
    "            pred_value = ner_model(input_) # a doc object, which contains the predicted entity\n",
    "            scorer.score(pred_value, gold) # calculate the accuracy\n",
    "        except:\n",
    "            textWithProblem.append((input_, annot))\n",
    "    return {n_iter: scorer.scores[k] for k in stats} # accumulate all the documents\n",
    "\n",
    "# find all entity type in the original data\n",
    "def findAllEntity(data = None):\n",
    "    uniqueTag = set()\n",
    "    [[uniqueTag.add(tag['TYPE']) for tag in eval(tags)] for tags in data['tags']]\n",
    "    return uniqueTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model=None, n_iter=3, output_dir=None):\n",
    "    \"\"\"Set up the pipeline and entity recognizer, and train the new entity.\"\"\"\n",
    "    random.seed(0)\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model) # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "    # Add entity recognizer to model if it's not in the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner)\n",
    "    # otherwise, get it, so we can add labels to it\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "    for LABEL in uniqueTag:\n",
    "        ner.add_label(LABEL)  # add new entity label to entity recognizer\n",
    "    # Adding extraneous labels shouldn't mess anything up\n",
    "#     ner.add_label(\"VEGETABLE\")\n",
    "    if model is None:\n",
    "        optimizer = nlp.begin_training()\n",
    "    else:\n",
    "        optimizer = nlp.resume_training()\n",
    "    move_names = list(ner.move_names)\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    dropout = decaying(0.6, 0.2, 1e-4)\n",
    "    batches = get_batches(train_data=TRAIN_DATA, model_type='ner')\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "#         sizes = compounding(4, 32, 1.001)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        for itn in range(n_iter):            \n",
    "            if itn%3 == 0:\n",
    "                random.shuffle(TRAIN_DATA)               \n",
    "                losses = {}\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    try:\n",
    "                        nlp.update(texts, annotations, sgd=optimizer, drop=next(dropout), losses=losses)\n",
    "                    except:\n",
    "                        textWithProblem.append(texts)\n",
    "                        pass\n",
    "                print(\"Losses\", losses)\n",
    "                if output_dir + str(itn) is not None:\n",
    "                    output_dir = Path(output_dir + str(itn))\n",
    "                    if not output_dir.exists():\n",
    "                        output_dir.mkdir()\n",
    "            #         nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "                    nlp.to_disk(output_dir)\n",
    "                    f1Train.append(evaluate(nlp, TRAIN_DATA, n_iter = itn))\n",
    "                    f1Test.append(evaluate(nlp, TEST_DATA, n_iter = itn))\n",
    "                    print(\"Saved model to\", output_dir)\n",
    "            else:\n",
    "                random.shuffle(TRAIN_DATA)\n",
    "                losses = {}\n",
    "                for batch in batches:\n",
    "                    texts, annotations = zip(*batch)\n",
    "                    try:\n",
    "                        nlp.update(texts, annotations, sgd=optimizer, drop=next(dropout), losses=losses)\n",
    "                    except:\n",
    "                        textWithProblem.append(texts)\n",
    "                        pass\n",
    "                f1Train.append(evaluate(nlp, TRAIN_DATA, n_iter = itn))\n",
    "                f1Test.append(evaluate(nlp, TEST_DATA, n_iter = itn))\n",
    "                print(\"Losses\", losses)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir + str(n_iter) is not None:\n",
    "        output_dir = Path(output_dir + str(n_iter))\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "#         nlp.meta[\"name\"] = new_model_name  # rename model\n",
    "        nlp.to_disk(output_dir)\n",
    "        f1Train.append(evaluate(nlp, TRAIN_DATA, n_iter = n_iter))\n",
    "        f1Test.append(evaluate(nlp, TEST_DATA, n_iter = n_iter))\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1Train = []\n",
    "f1Test = []\n",
    "df = pd.read_excel('./PythonExport.xlsx')\n",
    "df_2006Train = pd.read_excel('./i2b2-2006Train.xlsx')\n",
    "df_2006Test = pd.read_excel('./i2b2-2006Test.xlsx')\n",
    "df = pd.concat([df, df_2006Test, df_2006Train], axis = 0)\n",
    "train, test = train_test_split(df, test_size=0.1, random_state = 42)\n",
    "transformedTrainPerson = transformPerson(train)\n",
    "transformedTestPerson = transformPerson(test)\n",
    "uniqueTag = findAllEntityTransform(transformedTrainPerson)\n",
    "TRAIN_DATA = transformedTrainPerson\n",
    "TEST_DATA = transformedTestPerson\n",
    "textWithProblem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(n_iter = 3, output_dir='./emptyNameModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
